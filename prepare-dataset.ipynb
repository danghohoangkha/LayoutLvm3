{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import glob\n",
    "from pytesseract import pytesseract\n",
    "from lxml import etree\n",
    "import ast\n",
    "import torch\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, Features, ClassLabel, Sequence, Value, load_dataset\n",
    "from datasets import Image as DImage\n",
    "pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "from lxml import etree\n",
    "import keras_ocr\n",
    "\n",
    "pipeline = keras_ocr.pipeline.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelpath = './labeldata/project-4-at-2023-12-18-22-49-43cf6b2d.json'\n",
    "f = open(labelpath)\n",
    "label_studio_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_config = './Label_Config.xml'\n",
    "tree = etree.parse(label_config)\n",
    "root = tree.getroot()\n",
    "conf_labels = [label.get('value') for label in root.findall(\".//Label\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path, name):\n",
    "    image = cv2.imread(path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "    threshold_image = cv2.adaptiveThreshold(\n",
    "        blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "    save_path = './gray_image/' + name\n",
    "    cv2.imwrite(save_path, threshold_image)\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box_1, box_2):\n",
    "    poly_1 = Polygon(box_1)\n",
    "    poly_2 = Polygon(box_2)\n",
    "    # print(poly_1,poly_2)\n",
    "    # iou = poly_1.intersection(poly_2).area / poly_1.union(poly_2).area\n",
    "    iou = poly_1.intersection(poly_2).area\n",
    "    min_area = min(poly_1.area,poly_2.area)\n",
    "    return iou/min_area\n",
    "    \n",
    "    \n",
    "def hocr_to_dataframe(fp):\n",
    "    doc = etree.parse(fp)\n",
    "    words = []\n",
    "    wordConf = []\n",
    "    coords_list = []\n",
    "    for path in doc.xpath('//*'):\n",
    "        if 'ocrx_word' in path.values():\n",
    "            coord_text = path.values()[2].split(';')[0].split(' ')[1:] \n",
    "            word_coord = list(map(int, coord_text)) #x1, y1, x2, y2\n",
    "            conf = [x for x in path.values() if 'x_wconf' in x][0]\n",
    "            wordConf.append(int(conf.split('x_wconf ')[1]))\n",
    "            words.append(path.text)\n",
    "            coords_list.append(word_coord)\n",
    "\n",
    "    dfReturn = pd.DataFrame({'word' : words,\n",
    "                             'coords': coords_list,\n",
    "                             'confidence' : wordConf})\n",
    "\n",
    "    return(dfReturn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_data = dict()\n",
    "document_data['file_name'] = []\n",
    "document_data['labelled_bbox']= []\n",
    "\n",
    "for i in range(len(label_studio_data)):\n",
    "    row = label_studio_data[i]\n",
    "    upload_file_name = os.path.basename(row['data']['image'])\n",
    "    parts = upload_file_name.split('-')\n",
    "    file_name = '-'.join(parts[1:])\n",
    "    label_list, labels, bboxes = [], [], []\n",
    "\n",
    "    for label_ in row['annotations'][0]['result']:\n",
    "        label_value = label_['value']\n",
    "        x, y, w, h = label_value['x'], label_value['y'], label_value['width'], label_value['height']\n",
    "        original_w , original_h = label_['original_width'], label_['original_height']\n",
    "\n",
    "        x1 = int((x * original_w) / 100)\n",
    "        y1 = int((y * original_h) / 100)\n",
    "        x2 = x1 + int(original_w*w / 100)\n",
    "        y2 = y1 + int(original_h*h / 100)\n",
    "        \n",
    "        label = label_value['rectanglelabels']\n",
    "        label_list.append((label, (x1,y1,x2,y2), original_h, original_w))\n",
    "        \n",
    "    document_data['file_name'].append(file_name)    \n",
    "    document_data['labelled_bbox'].append(label_list)        \n",
    "\n",
    "custom_dataset = pd.DataFrame(document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {key: index for index, key in enumerate(conf_labels)}\n",
    "id2label = {v:k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    return image, (w,h)\n",
    "\n",
    "def normalize_bbox(bbox, size):\n",
    "    return [\n",
    "        int(1000 * bbox[0] / size[0]),\n",
    "        int(1000 * bbox[1] / size[1]),\n",
    "        int(1000 * bbox[2] / size[0]),\n",
    "        int(1000 * bbox[3] / size[1]),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_bounding_box(image_path, box):\n",
    "    extract_img = Image.open(image_path)\n",
    "    basename = os.path.basename(image_path)\n",
    "    cropped_image = extract_img.crop(box)\n",
    "    cropped_path = './cropped_image/' + basename\n",
    "    cropped_image.save(cropped_path)\n",
    "    cropped_image_array = keras_ocr.tools.read(cropped_path)\n",
    "    prediction_group = pipeline.recognize([cropped_image_array])\n",
    "    detected_texts = [word for word, box in prediction_group[0]]\n",
    "    paragraph = ' '.join(detected_texts)\n",
    "    return paragraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process label data - method 2\n",
    "final_list2 = []\n",
    "   \n",
    "for i in tqdm(custom_dataset.iterrows(), total=custom_dataset.shape[0]):\n",
    "    custom_label_text = {}\n",
    "    word_list = []\n",
    "    ner_tags_list  = []\n",
    "    bboxes_list = []\n",
    "    \n",
    "    file_name = i[1]['file_name']\n",
    "    for image in glob.glob('./image_bill/*.jpg'): #Make sure you add your extension or change it based on your needs \n",
    "        frame_file_name = os.path.basename(image)\n",
    "        if frame_file_name == file_name:\n",
    "            custom_label_text['id'] = i[0]\n",
    "            image_basename = os.path.basename(image)\n",
    "            annotations = []\n",
    "            # process image\n",
    "            gray_image = preprocess_image(image, image_basename)\n",
    "            label_coord_list = i[1]['labelled_bbox']\n",
    "\n",
    "            regconize_image = keras_ocr.tools.read(gray_image)\n",
    "            prediction_groups = pipeline.recognize([regconize_image])\n",
    "\n",
    "            for label_coord in label_coord_list:\n",
    "                (x1,y1,x2,y2) = label_coord[1]\n",
    "                box1 = [x1,y1,x2,y2] \n",
    "                label = label_coord[0][0]\n",
    "                extract_text = extract_text_from_bounding_box(gray_image, box1)\n",
    "                ner_tags = label2id[label]\n",
    "                bboxes_list.append(box1)\n",
    "                ner_tags_list.append(ner_tags)\n",
    "                word_list.append(extract_text)\n",
    "            dp_image, size = load_image(gray_image)\n",
    "            custom_label_text['image'] = dp_image\n",
    "            custom_label_text['words'] = word_list\n",
    "            custom_label_text['bboxes'] = [normalize_bbox(box, size) for box in bboxes_list]\n",
    "            custom_label_text['ner_tags'] = ner_tags_list\n",
    "    final_list2.append(custom_label_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "   \n",
    "for i in tqdm(custom_dataset.iterrows(), total=custom_dataset.shape[0]):\n",
    "    custom_label_text = {}\n",
    "    word_list = []\n",
    "    ner_tags_list  = []\n",
    "    bboxes_list = []\n",
    "    \n",
    "    file_name = i[1]['file_name']\n",
    "    for image in glob.glob('./image_bill/*.jpg'): #Make sure you add your extension or change it based on your needs \n",
    "        frame_file_name = os.path.basename(image)\n",
    "        if frame_file_name == file_name:\n",
    "            custom_label_text['id'] = i[0]\n",
    "            image_basename = os.path.basename(image)\n",
    "            annotations = []\n",
    "            # process image\n",
    "            gray_image = preprocess_image(image, image_basename)\n",
    "            label_coord_list = i[1]['labelled_bbox']\n",
    "\n",
    "            regconize_image = keras_ocr.tools.read(gray_image)\n",
    "            prediction_groups = pipeline.recognize([regconize_image])\n",
    "\n",
    "            for text, box in prediction_groups[0]:\n",
    "                for label_coord in label_coord_list:\n",
    "                    (x1,y1,x2,y2) = label_coord[1]\n",
    "                    box1 = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]] \n",
    "                    label = label_coord[0][0]\n",
    "                    box2 = box\n",
    "                    words = text\n",
    "                    overlap_perc = calculate_iou(box1,box2)\n",
    "                    if overlap_perc > 0.80:\n",
    "                        if words != '-':\n",
    "                            word_list.append(words)\n",
    "                            x1, y1 = min([coord[0] for coord in box]), min([coord[1] for coord in box])\n",
    "                            x2, y2 = max([coord[0] for coord in box]), max([coord[1] for coord in box])\n",
    "                            format_box = [x1, y1, x2, y2]\n",
    "                            bboxes_list.append(format_box)\n",
    "                            label_id = label2id[label]                              \n",
    "                            ner_tags_list.append(label_id)\n",
    "                        dp_image, size = load_image(gray_image)\n",
    "                        custom_label_text['image'] = dp_image\n",
    "                        custom_label_text['words'] = word_list\n",
    "                        custom_label_text['bboxes'] = [normalize_bbox(box, size) for box in bboxes_list]\n",
    "                        custom_label_text['ner_tags'] = ner_tags_list\n",
    "    final_list.append(custom_label_text)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    'id': Value('int64'),  # or 'string' if your ID is a string\n",
    "    'image': DImage(),  # Assuming image paths or URLs as strings\n",
    "    'words': Sequence(Value('string')),\n",
    "    'bboxes': Sequence(Sequence(Value('int64'))),  # Nested sequence for bounding box coordinates\n",
    "    'ner_tags': Sequence(ClassLabel(names=conf_labels)),  # Update with your actual NER tags\n",
    "})\n",
    "dataset = Dataset.from_list(final_list2, features=features)\n",
    "# dataset = Dataset.from_list(final_list, features=features)\n",
    "dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_box(bbox, width, height):\n",
    "    return [\n",
    "        int(width * (bbox[0] / 1000)),\n",
    "        int(height * (bbox[1] / 1000)),\n",
    "        int(width * (bbox[2] / 1000)),\n",
    "        int(height * (bbox[3] / 1000)),\n",
    "    ]\n",
    "example = dataset['train'][0]\n",
    "image = example['image']\n",
    "draw = ImageDraw.Draw(image)\n",
    "width, height = image.size\n",
    "for box in example['bboxes']:\n",
    "    draw.rectangle(unnormalize_box(box,width,height), width=4, outline='red')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6324b3a8ba784566b71b71d973f60054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62abf010bd704a09b8972186efe29e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_directory = './bill_dataset2'\n",
    "dataset.save_to_disk(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q datasets seqeval\n",
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "image_path = './gray_image/0001.jpg'\n",
    "image = keras_ocr.tools.read(image_path)\n",
    "\n",
    "# Each result is a tuple (box, text)\n",
    "predictions = pipeline.recognize([image])\n",
    "dp_image2 = Image.open(image_path)\n",
    "draw = ImageDraw.Draw(dp_image2)\n",
    "for text, box in predictions[0]:\n",
    "    x1, y1 = min([coord[0] for coord in box]), min([coord[1] for coord in box])\n",
    "    x2, y2 = max([coord[0] for coord in box]), max([coord[1] for coord in box])\n",
    "    box = (x1, y1, x2, y2)\n",
    "    draw.rectangle(box, outline='red', width=1)\n",
    "dp_image2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset['train'][3]\n",
    "image = example['image']\n",
    "draw = ImageDraw.Draw(image)\n",
    "width, height = image.size\n",
    "font = ImageFont.load_default()\n",
    "i = 0\n",
    "for i in range(0, len(example['bboxes'])):\n",
    "    print(id2label[example['ner_tags'][i]])\n",
    "    box = unnormalize_box(example['bboxes'][i],width,height)\n",
    "    draw.rectangle(box, width=1, outline='red')\n",
    "    draw.text((box[0] + 10, box[1] - 10), text=id2label[example['ner_tags'][i]], fill ='green', font=font)\n",
    "image.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_path1 = './Image_bill/noise2.jpg'\n",
    "preprocess_image(noise_path1, 'noise2.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
